apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ollama-pvc
  namespace: insightlearn
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ollama-nginx-config
  namespace: insightlearn
data:
  nginx.conf: |
    events {
      worker_connections 1024;
    }
    http {
      server {
        listen 11435 ssl;
        server_name ollama-service;

        ssl_certificate /etc/nginx/ssl/tls.crt;
        ssl_certificate_key /etc/nginx/ssl/tls.key;
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers HIGH:!aNULL:!MD5;

        location / {
          proxy_pass http://127.0.0.1:11434;
          proxy_http_version 1.1;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection 'upgrade';
          proxy_set_header Host localhost;
          proxy_set_header X-Real-IP 127.0.0.1;
          proxy_cache_bypass $http_upgrade;
          proxy_set_header X-Forwarded-For 127.0.0.1;
          proxy_set_header X-Forwarded-Proto $scheme;

          # Timeout settings for long-running LLM requests
          proxy_connect_timeout 300s;
          proxy_send_timeout 300s;
          proxy_read_timeout 300s;
        }
      }
    }
---
apiVersion: v1
kind: Service
metadata:
  name: ollama-service
  namespace: insightlearn
spec:
  selector:
    app: ollama
  ports:
  - port: 11434
    targetPort: 11434
    name: http
    protocol: TCP
  - port: 11435
    targetPort: 11435
    name: https
    protocol: TCP
  type: ClusterIP
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: ollama
  namespace: insightlearn
  labels:
    app: ollama
    version: "0.15.2"
spec:
  serviceName: ollama-service
  replicas: 1
  selector:
    matchLabels:
      app: ollama
  template:
    metadata:
      labels:
        app: ollama
      annotations:
        prometheus.io/scrape: "false"
    spec:
      containers:
      - name: ollama
        image: docker.io/ollama/ollama:0.15.2
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 11434
          name: http
        volumeMounts:
        - name: ollama-data
          mountPath: /root/.ollama
        resources:
          requests:
            memory: "2Gi"
            cpu: "500m"
          limits:
            memory: "16Gi"
            cpu: "6000m"
        env:
        - name: OLLAMA_HOST
          value: "0.0.0.0:11434"
        - name: OLLAMA_ORIGINS
          value: "*"
        # PERFORMANCE OPTIMIZATIONS for ClawdBot translation streaming
        - name: OLLAMA_KEEP_ALIVE
          value: "-1"  # Keep model loaded forever (no reload penalty)
        - name: OLLAMA_NUM_THREADS
          value: "6"   # CPU threads per request (balanced for parallel)
        - name: OLLAMA_NUM_PARALLEL
          value: "8"   # Allow 8 concurrent translation requests (workers)
        - name: OLLAMA_MAX_LOADED_MODELS
          value: "2"   # qwen2.5:3b (ClawdBot) + qwen2:0.5b (chatbot)
        - name: OLLAMA_MAX_QUEUE
          value: "256" # Large queue for burst translation workload
        - name: OLLAMA_FLASH_ATTENTION
          value: "1"   # Enable flash attention for faster inference
        - name: OLLAMA_DEBUG
          value: "0"   # Disable debug logging in production
        livenessProbe:
          httpGet:
            path: /api/tags
            port: 11434
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /api/tags
            port: 11434
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
      - name: nginx-tls-proxy
        image: docker.io/library/nginx:alpine
        ports:
        - containerPort: 11435
          name: https
        volumeMounts:
        - name: ollama-tls
          mountPath: /etc/nginx/ssl
          readOnly: true
        - name: nginx-config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
          readOnly: true
        resources:
          requests:
            memory: "64Mi"
            cpu: "100m"
          limits:
            memory: "128Mi"
            cpu: "200m"
      volumes:
      - name: ollama-data
        persistentVolumeClaim:
          claimName: ollama-pvc
      - name: ollama-tls
        secret:
          secretName: ollama-tls
      - name: nginx-config
        configMap:
          name: ollama-nginx-config
---
apiVersion: batch/v1
kind: Job
metadata:
  name: ollama-pull-qwen2
  namespace: insightlearn
spec:
  template:
    spec:
      restartPolicy: OnFailure
      containers:
      - name: ollama-pull
        image: docker.io/curlimages/curl:latest
        command:
        - /bin/sh
        - -c
        - |
          # Wait for Ollama to be ready (HTTP on port 11434)
          until curl -s http://ollama-service:11434/api/version > /dev/null; do
            echo "Waiting for Ollama to start..."
            sleep 5
          done

          # Pull qwen2:0.5b model (ultra-fast for chatbots, 5-10s responses)
          curl -X POST http://ollama-service:11434/api/pull \
            -H "Content-Type: application/json" \
            -d '{"name": "qwen2:0.5b"}'

          echo "qwen2:0.5b model pulled successfully"
